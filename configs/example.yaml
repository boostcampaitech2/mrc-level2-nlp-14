DataArguments:

  # DataPathArguments
  dataset_name: ./data/aistage-mrc/train_dataset
  dataset_path: ./data/aistage-mrc
  context_path: wiki_800.json
  overwrite_cache: True
  preprocessing_num_workers: null
  curriculum_split_name: train_mask_2
  curriculum_learn: False
  
  # TokenizerArguments
  max_seq_length: 384
  pad_to_max_length: True
  doc_stride: 128
  max_answer_length: 30
  return_token_type_ids: False
  
  # RetrievalArguments
  retrieval_mode: elastic_engine
  retrieval_name: elastic_search
  rebuilt_index: False
  retrieval_tokenizer_name: mecab
  sp_max_features: 50000
  sp_ngram_range: [1,2]
  top_k_retrieval: 10
  use_faiss: False
  eval_retrieval: True
  num_clusters: 64
  
  # ElasticSearchArguments
  index_name: wiki-index
  stopword_path: user_dic/my_stop_dic.txt
  decompound_mode: mixed
  b: 0.5
  k1: 1.3
  es_host_address: localhost:9200
  es_similarity: bm25_similarity
  use_korean_stopwords: False
  use_korean_synonyms: False
  lowercase: False
  nori_readingform: False
  cjk_bigram: False
  decimal_digit: False
  dfr_basic_model: g
  dfr_after_effect: l
  es_normalization: h2
  dfi_measure: standardized
  ib_distribution: ll
  ib_lambda: df
  lmd_mu: 2000
  lmjm_lambda: 0.1
  
ModelArguments:

  # ModelArguments
<<<<<<< Updated upstream
  model_name_or_path: klue/roberta-large
=======
  model_name_or_path: ./outputs/layer_2_1900
>>>>>>> Stashed changes
  reader_type: extractive
  architectures: RobertaForQAWithConvSDSHead
  config_name: null
  tokenizer_name: null
  model_cache_dir: cache
  model_init: qaconv_head
<<<<<<< Updated upstream
=======
  # use_auth_token: True
  # revision: None
>>>>>>> Stashed changes
  
  # ModelHeadArguments
  model_head: sds_conv
  qa_conv_out_channel: 1024
  qa_conv_input_size: 384
  qa_conv_n_layers: 5
  use_auth_token: True
  
TrainingArguments:

  # HfTrainingArguments
  report_to: wandb
<<<<<<< Updated upstream
  run_name: run_test
  output_dir: outputs/run_test_2
=======
  run_name: v1.4.4_train_shuffle
  output_dir: outputs/wiki_pororo_ver2_non_ner
>>>>>>> Stashed changes
  overwrite_output_dir: False
  learning_rate: 2e-6
  do_train: True
  do_eval: True
  do_predict: True # eval_retrieval = True 필수
<<<<<<< Updated upstream
  do_pos_ensemble: False
  evaluation_strategy: epoch
  save_strategy: epoch
  per_device_train_batch_size: 16
=======
  evaluation_strategy: steps
  save_strategy: steps
  per_device_train_batch_size: 20
>>>>>>> Stashed changes
  per_device_eval_batch_size: 32
  num_train_epochs: 1
  eval_steps: 100
  save_steps: 100
  save_total_limit: 3
  fp16: True
  weight_decay: 0.01
  warmup_steps: 200
  load_best_model_at_end: True
  metric_for_best_model: exact_match
  logging_dir: logs
  lr_scheduler_type: cosine_with_restarts

  # Seq2SeqTrainingArguments
  sortish_sampler: False
  predict_with_generate: False
  generation_max_length: null
  generation_num_beams: null
  

ProjectArguments:

  # AnalyzerArguments
  wandb_project: klue_mrc_yj
  checkpoint: null