# Data Arguments
DataArguments:
  dataset_name: ./data/aistage-mrc/train_dataset
  overwrite_cache: True
  preprocessing_num_workers: null
  max_seq_length: 384
  pad_to_max_length: False
  doc_stride: 128
  max_answer_length: 30
  eval_retrieval: False
  num_clusters: 64
  top_k_retrieval: 1
  use_faiss: True

# TrainingArguments:
NewTrainingArguments:
  # trainer_class: QuestionAnsweringTrainer
  report_to: wandb
  run_name: klue/roberta-large_ws500_lr1e-5
  output_dir: outputs/klue-roberta-large
  overwrite_output_dir: False
  learning_rate: 1e-5
  do_train: True
  do_eval: True
  do_predict: False # eval_retrieval = True 필수
  evaluation_strategy: epoch
  save_strategy: epoch
  per_device_train_batch_size: 16
  per_device_eval_batch_size: 32
  num_train_epochs: 15
  # eval_steps: 100
  # save_steps: 100
  save_total_limit: 3
  fp16: True
  weight_decay: 0.01
  warmup_steps: 500
  load_best_model_at_end: True
  metric_for_best_model: exact_match
  logging_dir: logs
  lr_scheduler_type: cosine

# Model Arguments
ModelingArguments:
  model_name_or_path: klue/roberta-large
  architectures: RobertaForQuestionAnswering
  method: ext
  config_name: null
  tokenizer_name: klue/roberta-large
  model_cache_dir: cache
  model_init: basic
  
# Project Arguments
ProjectArguments:
  task: mrc
  wandb_project: klue_mrc
  save_model_dir: best
  checkpoint: None